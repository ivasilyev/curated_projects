---
apiVersion: batch/v1
kind: Job
metadata:
  name: count-correlations
  labels:
    tier: job
spec:
  # Get the required node number:
  # wc -l /data1/bio/projects/ashestopalov/nutrition/obesity_metagenomes/data/correlation_data/group_datasets/tables.txt.bak | awk '{print $1}'
  parallelism: 112
  template:
    metadata:
      name: count-correlations-job
      labels:
        tier: worker
    spec:
      containers:
        - name: curated-projects
          image: docker.io/ivasilyev/curated_projects:latest
          imagePullPolicy: Always
          command: ["python3", "/home/docker/scripts/curated_projects/ashestopalov/nutrition/obesity_metagenomes/5_count_correlations.py"]
          volumeMounts:
            - name: data1
              mountPath: /data1
      volumes:
        - name: data1
          hostPath:
            path: /data1
      affinity:
        # The table parsing eats about 90 Gb of RAM o_O
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 1
              preference:
                matchExpressions:
                  - key: ram
                    operator: In
                    values:
                    - "128"
      restartPolicy: Never

# Restore queue
# cp -r /data1/bio/projects/ashestopalov/nutrition/obesity_metagenomes/data/correlation_data/group_datasets/tables.txt.bak /data1/bio/projects/ashestopalov/nutrition/obesity_metagenomes/data/correlation_data/group_datasets/tables.txt

# Deploy from master node:
# export JOB_NAME="count-correlations" && kubectl create -f ~/k8s/count_correlations.yml

# Get pods info:
# kubectl get pods

# Get node info:
# kubectl describe pods "${JOB_NAME}" | grep Node:

# Cleanup:
# kubectl delete job "${JOB_NAME}"
# kubectl get pods | grep "${JOB_NAME}" | awk '{print $1}' | xargs -I {} kubectl delete pod {}
