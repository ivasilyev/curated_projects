---
apiVersion: batch/v1
kind: Job
metadata:
  name: count-correlations
  labels:
    tier: job
spec:
  # Get the required node number:
  # wc -l /data1/bio/projects/ashestopalov/nutrition/obesity_metagenomes/data/correlation_data/group_datasets/tables.txt.bak | awk '{print $1}'
  parallelism: 120
  template:
    metadata:
      name: count-correlations-job
      labels:
        tier: worker
    spec:
      containers:
        - name: curated-projects
          image: docker.io/ivasilyev/curated_projects:latest
          imagePullPolicy: Always
          command: ["/bin/bash", "-c"]
          args: ["git pull && python3 ashestopalov/nutrition/obesity_metagenomes/5_count_correlations.py"]
          volumeMounts:
            - name: data1
              mountPath: /data1
      volumes:
        - name: data1
          hostPath:
            path: /data1
      affinity:
        # The table parsing eats about 90 Gb of RAM o_O
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 1
              preference:
                matchExpressions:
                  - key: ram
                    operator: In
                    values:
                    - "128"
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels:
                tier: worker
      restartPolicy: Never

# Declare variables
# JOB_NAME="count-correlations" && BACKUP="/data1/bio/projects/ashestopalov/nutrition/obesity_metagenomes/data/correlation_data/group_datasets/tables.txt.bak" && QUEUE="/data1/bio/projects/ashestopalov/nutrition/obesity_metagenomes/data/correlation_data/group_datasets/tables.txt"

# Restore & shuffle the queue
# shuf < "${BACKUP}" > "${QUEUE}"

# Or select only the required pairs
# grep -E 'Indoles|Resorcinols' "${BACKUP}" | shuf > "${QUEUE}"

# Deploy from master node:
# kubectl create -f ~/k8s/count_correlations.yml

# View nodes:
# kubectl get nodes

# Get overall progress info:
# kubectl get pods | grep Completed | wc -l

# Get node info:
# kubectl describe pods "${JOB_NAME}" | grep Node:

# Cleanup:
# kubectl delete job "${JOB_NAME}"
# kubectl get pods | grep "${JOB_NAME}" | awk '{print $1}' | xargs -I {} kubectl delete pod {}
