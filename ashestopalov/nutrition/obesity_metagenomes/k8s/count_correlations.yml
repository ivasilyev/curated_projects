---
apiVersion: batch/v1
kind: Job
metadata:
  name: count-correlation
  labels:
    tier: job
spec:
  # Get nodes number: wc -l /data1/bio/projects/ashestopalov/nutrition/obesity_metagenomes/data/correlation_data/group_datasets/tables.txt.bak
  parallelism: 4
  template:
    metadata:
      name: count-correlations-job
      labels:
        tier: worker
    spec:
      containers:
        - name: curated-projects
          image: docker.io/ivasilyev/curated_projects:latest
          imagePullPolicy: Always
          command: ["python3", "/home/docker/scripts/curated_projects/ashestopalov/nutrition/obesity_metagenomes/5_count_correlations.py"]
          volumeMounts:
            - name: data1
              mountPath: /data1
      volumes:
        - name: data1
          hostPath:
            path: /data1
      affinity:
        # The table parsing eats about 90 Gb of RAM o_O
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 1
              preference:
                matchExpressions:
                  - key: ram
                    operator: In
                    values:
                    - "128"
      restartPolicy: Never

# Restore queue
# cp -r /data1/bio/projects/ashestopalov/nutrition/obesity_metagenomes/data/correlation_data/group_datasets/tables.txt.bak /data1/bio/projects/ashestopalov/nutrition/obesity_metagenomes/data/correlation_data/group_datasets/tables.txt

# Deploy from master node:
# kubectl create -f ~/k8s/count_correlations.yml

# Get pods info:
# kubectl get pods

# Get node info:
# kubectl describe pod count-correlation | grep Node:

# Get token:
# kubectl logs count-correlation | grep token

# Cleanup:
# kubectl get pods | grep count-correlation | awk '{print $1}' | xargs -I {} kubectl delete pod {}
